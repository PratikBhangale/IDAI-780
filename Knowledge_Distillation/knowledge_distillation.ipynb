{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 21:16:08,834 - utils - INFO - ModelManager initialized with device: cuda\n",
      "2025-04-06 21:16:08,834 - utils - INFO - Loading model: meta-llama/Llama-3.1-8B-Instruct (teacher: True)\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da69f60e0194972a6cbf797d4f4885f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 21:16:17,076 - utils - INFO - Applied torch.compile to model\n",
      "2025-04-06 21:16:17,077 - utils - INFO - Successfully loaded model: meta-llama/Llama-3.1-8B-Instruct\n",
      "2025-04-06 21:16:17,079 - utils - INFO - GPU Memory: 8.46 GB allocated, 8.51 GB reserved\n",
      "2025-04-06 21:16:17,079 - utils - INFO - Loading tokenizer for: meta-llama/Llama-3.1-8B-Instruct\n",
      "2025-04-06 21:16:17,419 - utils - INFO - Successfully loaded tokenizer for: meta-llama/Llama-3.1-8B-Instruct\n",
      "2025-04-06 21:16:17,421 - utils - INFO - Loading model: meta-llama/Llama-3.2-3B-Instruct (teacher: False)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e92295ec2684fa19930cc4ebd4dbcbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 21:16:23,257 - utils - INFO - Successfully loaded model: meta-llama/Llama-3.2-3B-Instruct\n",
      "2025-04-06 21:16:23,257 - utils - INFO - GPU Memory: 14.45 GB allocated, 14.51 GB reserved\n",
      "2025-04-06 21:16:23,257 - utils - INFO - Loading tokenizer for: meta-llama/Llama-3.2-3B-Instruct\n",
      "2025-04-06 21:16:23,653 - utils - INFO - Successfully loaded tokenizer for: meta-llama/Llama-3.2-3B-Instruct\n",
      "2025-04-06 21:16:23,653 - utils - INFO - MMLUMedicalDataset initialized with max_length: 512\n",
      "2025-04-06 21:16:23,654 - utils - INFO - Loading HPAI-BSC/MMLU-medical-cot-llama31 dataset\n",
      "2025-04-06 21:16:24,576 - utils - INFO - Dataset loaded successfully\n",
      "2025-04-06 21:16:24,577 - utils - INFO - Preparing dataset for knowledge distillation\n",
      "2025-04-06 21:16:24,577 - utils - INFO - Using streaming mode - data will be processed on-the-fly\n",
      "2025-04-06 21:16:24,578 - utils - INFO - Dataset prepared for distillation\n",
      "2025-04-06 21:16:24,579 - utils - WARNING - Teacher model is on cuda:0, but distillation is configured for cuda\n",
      "2025-04-06 21:16:24,580 - utils - WARNING - Student model is on cuda:0, but distillation is configured for cuda\n",
      "2025-04-06 21:16:24,580 - utils - INFO - Knowledge Distillation initialized with temperature: 2.0, alpha: 0.5, device: cuda, dtype: torch.float16\n",
      "s:\\Projects\\Capstone\\beta_1\\Knowledge_Distillation\\utils.py:842: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler() if self.use_mixed_precision else None\n",
      "2025-04-06 21:16:24,581 - utils - INFO - DistillationTrainer initialized with mixed precision: True, gradient accumulation steps: 4\n",
      "2025-04-06 21:16:24,581 - utils - WARNING - No validation data available\n",
      "2025-04-06 21:16:24,582 - utils - INFO - Starting training for 3 epochs\n",
      "2025-04-06 21:16:24,582 - utils - INFO - Epoch 1/3\n",
      "Training epoch 1: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'answer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 81\u001b[0m\n\u001b[0;32m     71\u001b[0m trainer \u001b[38;5;241m=\u001b[39m DistillationTrainer(\n\u001b[0;32m     72\u001b[0m     distillation\u001b[38;5;241m=\u001b[39mdistillation,\n\u001b[0;32m     73\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m     grad_accum_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m\n\u001b[0;32m     78\u001b[0m )\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# 9. Train the model\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./checkpoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to 0 to avoid multiprocessing issues\u001b[39;49;00m\n\u001b[0;32m     88\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32ms:\\Projects\\Capstone\\beta_1\\Knowledge_Distillation\\utils.py:921\u001b[0m, in \u001b[0;36mDistillationTrainer.train\u001b[1;34m(self, epochs, batch_size, checkpoint_dir, eval_steps, save_steps, max_grad_norm, log_steps, num_workers)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistillation\u001b[38;5;241m.\u001b[39mstudent_model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    919\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(train_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(progress_bar):\n\u001b[0;32m    922\u001b[0m     \u001b[38;5;66;03m# Determine if this is the last accumulation step\u001b[39;00m\n\u001b[0;32m    923\u001b[0m     is_last_accum_step \u001b[38;5;241m=\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_accum_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;66;03m# Mixed precision context\u001b[39;00m\n",
      "File \u001b[1;32ms:\\Virtual_Enviornments\\chatbot\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32ms:\\Virtual_Enviornments\\chatbot\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32ms:\\Virtual_Enviornments\\chatbot\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32ms:\\Virtual_Enviornments\\chatbot\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:43\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_iter)\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32ms:\\Projects\\Capstone\\beta_1\\Knowledge_Distillation\\utils.py:383\u001b[0m, in \u001b[0;36mmedical_dataset_collate_fn\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;66;03m# Handle streaming data\u001b[39;00m\n\u001b[0;32m    382\u001b[0m     input_texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[1;32m--> 383\u001b[0m     output_texts \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;66;03m# Include CoT if available\u001b[39;00m\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch):\n",
      "File \u001b[1;32ms:\\Projects\\Capstone\\beta_1\\Knowledge_Distillation\\utils.py:383\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;66;03m# Handle streaming data\u001b[39;00m\n\u001b[0;32m    382\u001b[0m     input_texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[1;32m--> 383\u001b[0m     output_texts \u001b[38;5;241m=\u001b[39m [\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43manswer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;66;03m# Include CoT if available\u001b[39;00m\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch):\n",
      "\u001b[1;31mKeyError\u001b[0m: 'answer'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from utils import (\n",
    "    load_env_variables,\n",
    "    ModelManager,\n",
    "    MMLUMedicalDataset,\n",
    "    VanillaKnowledgeDistillation,\n",
    "    DistillationTrainer,\n",
    "    Evaluator\n",
    ")\n",
    "\n",
    "\n",
    "# 1. Load environment variables\n",
    "api_key = load_env_variables()\n",
    "\n",
    "# 2. Initialize model manager\n",
    "model_manager = ModelManager(api_key=api_key)\n",
    "\n",
    "# 3. Load teacher model (larger model)\n",
    "teacher_model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "teacher_model = model_manager.load_model(\n",
    "    model_name=teacher_model_name,\n",
    "    is_teacher=True,\n",
    "    use_8bit=True\n",
    ")\n",
    "teacher_tokenizer = model_manager.load_tokenizer(model_name=teacher_model_name)\n",
    "\n",
    "# 4. Load student model (smaller model)\n",
    "student_model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "student_model = model_manager.load_model(\n",
    "    model_name=student_model_name,\n",
    "    is_teacher=False\n",
    ")\n",
    "student_tokenizer = model_manager.load_tokenizer(model_name=student_model_name)\n",
    "\n",
    "# 5. Initialize dataset\n",
    "dataset = MMLUMedicalDataset(\n",
    "    tokenizer=teacher_tokenizer,\n",
    "    max_length=512,\n",
    "    streaming=True\n",
    ")\n",
    "dataset.load_data()\n",
    "dataset.prepare_for_distillation()\n",
    "\n",
    "# 6. Initialize optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(\n",
    "    student_model.parameters(),\n",
    "    lr=5e-5,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "num_training_steps = 10000\n",
    "num_warmup_steps = 1000\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# 7. Initialize knowledge distillation\n",
    "distillation = VanillaKnowledgeDistillation(\n",
    "    teacher_model=teacher_model,\n",
    "    student_model=student_model,\n",
    "    teacher_tokenizer=teacher_tokenizer,\n",
    "    student_tokenizer=student_tokenizer,\n",
    "    temperature=2.0,\n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "# 8. Initialize trainer\n",
    "trainer = DistillationTrainer(\n",
    "    distillation=distillation,\n",
    "    dataset=dataset,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    use_mixed_precision=True,\n",
    "    grad_accum_steps=4\n",
    ")\n",
    "\n",
    "# 9. Train the model\n",
    "metrics = trainer.train(\n",
    "    epochs=3,\n",
    "    batch_size=8,\n",
    "    checkpoint_dir=\"./checkpoints\",\n",
    "    eval_steps=500,\n",
    "    save_steps=1000,\n",
    "    num_workers=0  # Set to 0 to avoid multiprocessing issues\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Evaluate the distilled model\n",
    "evaluator = Evaluator(\n",
    "    model=student_model,\n",
    "    tokenizer=student_tokenizer,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# Save the final distilled model\n",
    "student_model.save_pretrained(\"./distilled_model\")\n",
    "student_tokenizer.save_pretrained(\"./distilled_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "test_data = [\n",
    "    {\n",
    "        'input': \"Question: What is the most common cause of community-acquired pneumonia?\",\n",
    "        'reference': \"The most common cause of community-acquired pneumonia is Streptococcus pneumoniae.\"\n",
    "    }\n",
    "    # Add more test examples as needed\n",
    "]\n",
    "\n",
    "# Run evaluation\n",
    "eval_results = evaluator.evaluate_model(test_data)\n",
    "print(f\"Perplexity: {eval_results['perplexity']:.4f}\")\n",
    "print(f\"METEOR score: {eval_results['meteor']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
