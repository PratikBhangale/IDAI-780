{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Distillation: Llama 3.2 3B → Llama 3.2 1B\n",
    "\n",
    "This notebook implements knowledge distillation from a teacher model (Llama 3.2 3B) to a student model (Llama 3.2 1B) using the MMLU-medical-cot-llama31 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Total GPU memory: 12.88 GB\n",
      "Available GPU memory: 0.00 GB reserved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prati\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\prati\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import logging\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Download necessary NLTK data\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to download NLTK data: {e}\")\n",
    "\n",
    "# Ensure using GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Check CUDA memory\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"Available GPU memory: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB reserved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models and Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model paths - use local paths if models are already downloaded\n",
    "student_model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "teacher_model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "# Maximum sequence length for tokenization\n",
    "max_length = 512\n",
    "\n",
    "# Authentication for Hugging Face (required for Meta-Llama models)\n",
    "# You need to set your HF_TOKEN in the environment or pass it directly\n",
    "# os.environ[\"HF_TOKEN\"] = \"your_hugging_face_token\"  # Uncomment and set your token\n",
    "hf_token = os.environ.get(\"HF_TOKEN\", None)\n",
    "\n",
    "try:\n",
    "    # Load the student model (Llama 3.2 1B) with gradient checkpointing for memory efficiency\n",
    "    print(\"Loading student model and tokenizer...\")\n",
    "    student_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        student_model_name, \n",
    "        token=hf_token,\n",
    "        use_fast=True\n",
    "    )\n",
    "    student_model = AutoModelForCausalLM.from_pretrained(\n",
    "        student_model_name,\n",
    "        token=hf_token,\n",
    "        device_map=\"auto\",  # Automatically determine best device mapping\n",
    "        torch_dtype=torch.float16,  # Use half precision to save memory\n",
    "        use_cache=False  # Disable KV cache during training\n",
    "    )\n",
    "    student_model.gradient_checkpointing_enable()  # Enable gradient checkpointing to save memory\n",
    "    \n",
    "    # Set pad token if not defined\n",
    "    if student_tokenizer.pad_token_id is None:\n",
    "        student_tokenizer.pad_token_id = student_tokenizer.eos_token_id\n",
    "        \n",
    "    print(\"Student model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load student model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the teacher model (Llama 3.2 3B)\n",
    "    print(\"Loading teacher model and tokenizer...\")\n",
    "    teacher_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        teacher_model_name,\n",
    "        token=hf_token,\n",
    "        use_fast=True\n",
    "    )\n",
    "    teacher_model = AutoModelForCausalLM.from_pretrained(\n",
    "        teacher_model_name,\n",
    "        token=hf_token,\n",
    "        device_map=\"auto\",  # Automatically determine best device mapping\n",
    "        torch_dtype=torch.float16,  # Use half precision to save memory\n",
    "    )\n",
    "    \n",
    "    # Set teacher model to evaluation mode\n",
    "    teacher_model.eval()\n",
    "    \n",
    "    # Set pad token if not defined\n",
    "    if teacher_tokenizer.pad_token_id is None:\n",
    "        teacher_tokenizer.pad_token_id = teacher_tokenizer.eos_token_id\n",
    "        \n",
    "    print(\"Teacher model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load teacher model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the MMLU-medical-cot-llama31 dataset\n",
    "    dataset_name = \"HPAI-BSC/MMLU-medical-cot-llama31\"\n",
    "    print(f\"Loading dataset: {dataset_name}\")\n",
    "    dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "    # Select a small subset for faster experimentation (can increase later)\n",
    "    subset_size = 100\n",
    "    subset = dataset.select(range(subset_size))\n",
    "    print(f\"Loaded {len(subset)} examples from {dataset_name}\")\n",
    "\n",
    "    # Display an example from the dataset\n",
    "    example = subset[0]\n",
    "    print(\"\\nExample input:\")\n",
    "    print(f\"{example['question'][:300]}{'...' if len(example['question']) > 300 else ''}\")\n",
    "    print(\"\\nExample response:\")\n",
    "    print(f\"{example['response'][:300]}{'...' if len(example['response']) > 300 else ''}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset for Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(batch_data, student_tokenizer, teacher_tokenizer, max_length=512, device=device):\n",
    "    \"\"\"\n",
    "    Prepare a batch of data for training with proper padding and attention masks.\n",
    "    \"\"\"\n",
    "    # Extract questions and responses\n",
    "    questions = [item[\"question\"] for item in batch_data]\n",
    "    responses = [item[\"response\"] for item in batch_data]\n",
    "    \n",
    "    # Tokenize inputs (questions) with padding\n",
    "    encoded_inputs = student_tokenizer(\n",
    "        questions,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Tokenize targets (responses) with padding\n",
    "    encoded_targets = student_tokenizer(\n",
    "        responses,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Also tokenize inputs with teacher tokenizer if different\n",
    "    if teacher_tokenizer != student_tokenizer:\n",
    "        teacher_encoded_inputs = teacher_tokenizer(\n",
    "            questions,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "    else:\n",
    "        teacher_encoded_inputs = encoded_inputs\n",
    "    \n",
    "    return {\n",
    "        \"student_input_ids\": encoded_inputs.input_ids,\n",
    "        \"student_attention_mask\": encoded_inputs.attention_mask,\n",
    "        \"teacher_input_ids\": teacher_encoded_inputs.input_ids,\n",
    "        \"teacher_attention_mask\": teacher_encoded_inputs.attention_mask,\n",
    "        \"target_input_ids\": encoded_targets.input_ids,\n",
    "        \"target_attention_mask\": encoded_targets.attention_mask\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, dataset, max_length=512, max_new_tokens=50, batch_size=4):\n",
    "    \"\"\"\n",
    "    Evaluate model using METEOR score and perplexity metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_meteor_score = 0\n",
    "    total_perplexity = 0\n",
    "    total = 0\n",
    "\n",
    "    try:\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            batch_data = dataset[i:i+batch_size]\n",
    "            \n",
    "            for data in tqdm(batch_data, desc=\"Evaluating\"):\n",
    "                prompt = data[\"question\"]\n",
    "                correct_answer = data[\"response\"]\n",
    "\n",
    "                # Generate model's response\n",
    "                try:\n",
    "                    inputs = tokenizer(\n",
    "                        prompt, \n",
    "                        return_tensors=\"pt\", \n",
    "                        truncation=True, \n",
    "                        max_length=max_length\n",
    "                    ).to(device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        outputs = model.generate(\n",
    "                            input_ids=inputs.input_ids,\n",
    "                            attention_mask=inputs.attention_mask,\n",
    "                            max_new_tokens=max_new_tokens,\n",
    "                            do_sample=False\n",
    "                        )\n",
    "                    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "                    # Calculate METEOR score\n",
    "                    try:\n",
    "                        reference_tokens = word_tokenize(correct_answer.lower())\n",
    "                        candidate_tokens = word_tokenize(generated_text.lower())\n",
    "                        \n",
    "                        # Check for empty token lists\n",
    "                        if len(reference_tokens) == 0 or len(candidate_tokens) == 0:\n",
    "                            print(\"Warning: Empty token list detected, skipping METEOR calculation\")\n",
    "                            meteor = 0\n",
    "                        else:\n",
    "                            meteor = meteor_score([reference_tokens], candidate_tokens)\n",
    "                        \n",
    "                        total_meteor_score += meteor\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error calculating METEOR score: {e}\")\n",
    "                        meteor = 0\n",
    "\n",
    "                    # Calculate perplexity\n",
    "                    try:\n",
    "                        target_encoding = tokenizer(\n",
    "                            correct_answer, \n",
    "                            return_tensors=\"pt\",\n",
    "                            truncation=True,\n",
    "                            max_length=max_length\n",
    "                        ).to(device)\n",
    "                        \n",
    "                        target_ids = target_encoding.input_ids\n",
    "                        \n",
    "                        with torch.no_grad():\n",
    "                            outputs = model(input_ids=target_ids)\n",
    "                            logits = outputs.logits\n",
    "                            \n",
    "                        shift_logits = logits[..., :-1, :].contiguous()\n",
    "                        shift_labels = target_ids[..., 1:].contiguous()\n",
    "                        \n",
    "                        loss_fct = torch.nn.CrossEntropyLoss(\n",
    "                            ignore_index=tokenizer.pad_token_id, \n",
    "                            reduction='sum'\n",
    "                        )\n",
    "                        \n",
    "                        loss = loss_fct(\n",
    "                            shift_logits.view(-1, shift_logits.size(-1)), \n",
    "                            shift_labels.view(-1)\n",
    "                        )\n",
    "                        \n",
    "                        num_tokens = (shift_labels != tokenizer.pad_token_id).sum().item()\n",
    "                        if num_tokens > 0:\n",
    "                            perplexity = torch.exp(loss / num_tokens).item()\n",
    "                        else:\n",
    "                            perplexity = float('inf')\n",
    "                            print(\"Warning: No valid tokens for perplexity calculation\")\n",
    "                            \n",
    "                        total_perplexity += perplexity\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error calculating perplexity: {e}\")\n",
    "                        perplexity = float('inf')\n",
    "                        \n",
    "                    total += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during evaluation: {e}\")\n",
    "                    continue\n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation failed: {e}\")\n",
    "\n",
    "    # Calculate averages\n",
    "    if total > 0:\n",
    "        average_meteor_score = total_meteor_score / total\n",
    "        average_perplexity = total_perplexity / total\n",
    "    else:\n",
    "        average_meteor_score = 0\n",
    "        average_perplexity = float('inf')\n",
    "        print(\"Warning: No examples were successfully evaluated\")\n",
    "\n",
    "    return average_meteor_score, average_perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Evaluating student model before knowledge distillation...\")\n",
    "    initial_meteor, initial_perplexity = evaluate_model(\n",
    "        student_model, \n",
    "        student_tokenizer, \n",
    "        subset, \n",
    "        max_length=max_length,\n",
    "        max_new_tokens=50,\n",
    "        batch_size=4\n",
    "    )\n",
    "    print(f\"Initial Student Model - Average METEOR Score: {initial_meteor * 100:.2f}%\")\n",
    "    print(f\"Initial Student Model - Average Perplexity: {initial_perplexity:.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Baseline evaluation failed: {e}\")\n",
    "    initial_meteor, initial_perplexity = 0, float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge Distillation parameters\n",
    "alpha = 0.5  # Weight for KL divergence loss\n",
    "temperature = 2.0  # Temperature for softening probability distributions\n",
    "num_epochs = 3\n",
    "learning_rate = 5e-5\n",
    "batch_size = 4\n",
    "gradient_accumulation_steps = 4  # Accumulate gradients to simulate larger batch sizes\n",
    "warmup_steps = 100\n",
    "\n",
    "# Create an optimizer for the student model\n",
    "optimizer = torch.optim.AdamW(student_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create a learning rate scheduler\n",
    "total_steps = (len(subset) // batch_size) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=warmup_steps, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Function to get teacher logits for a batch\n",
    "def get_teacher_logits(input_ids, attention_mask=None):\n",
    "    with torch.no_grad():\n",
    "        teacher_outputs = teacher_model(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        return teacher_outputs.logits\n",
    "\n",
    "# Create checkpoint directory\n",
    "checkpoint_dir = Path(\"checkpoints\")\n",
    "checkpoint_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with knowledge distillation\n",
    "print(\"Starting knowledge distillation training...\")\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        student_model.train()\n",
    "        total_loss = 0\n",
    "        batches = 0\n",
    "        \n",
    "        # Create batches\n",
    "        progress_bar = tqdm(range(0, len(subset), batch_size), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for i in progress_bar:\n",
    "            batch_data = subset[i:i+batch_size]\n",
    "            \n",
    "            # Prepare batch data\n",
    "            batch = prepare_batch(\n",
    "                batch_data, \n",
    "                student_tokenizer, \n",
    "                teacher_tokenizer, \n",
    "                max_length=max_length\n",
    "            )\n",
    "            \n",
    "            # Get student logits\n",
    "            student_outputs = student_model(\n",
    "                input_ids=batch[\"student_input_ids\"],\n",
    "                attention_mask=batch[\"student_attention_mask\"]\n",
    "            )\n",
    "            student_logits = student_outputs.logits\n",
    "            \n",
    "            # Get teacher logits for the same input\n",
    "            teacher_logits = get_teacher_logits(\n",
    "                batch[\"teacher_input_ids\"],\n",
    "                attention_mask=batch[\"teacher_attention_mask\"]\n",
    "            )\n",
    "            \n",
    "            # Hard targets loss (cross-entropy with true labels)\n",
    "            # Shift logits and labels for next token prediction\n",
    "            shift_logits = student_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = batch[\"target_input_ids\"][..., 1:].contiguous()\n",
    "            \n",
    "            hard_loss_fct = torch.nn.CrossEntropyLoss(\n",
    "                ignore_index=student_tokenizer.pad_token_id\n",
    "            )\n",
    "            \n",
    "            hard_loss = hard_loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                shift_labels.view(-1)\n",
    "            )\n",
    "            \n",
    "            # Soft targets loss (KL divergence with teacher outputs)\n",
    "            # Ensure student and teacher logits have the same shape\n",
    "            # We'll use the first min(student_len, teacher_len) tokens\n",
    "            min_length = min(student_logits.size(1), teacher_logits.size(1))\n",
    "            \n",
    "            student_logits_t = F.log_softmax(\n",
    "                student_logits[:, :min_length, :] / temperature, \n",
    "                dim=-1\n",
    "            )\n",
    "            \n",
    "            teacher_logits_t = F.softmax(\n",
    "                teacher_logits[:, :min_length, :] / temperature, \n",
    "                dim=-1\n",
    "            )\n",
    "            \n",
    "            # KL divergence loss\n",
    "            soft_loss_fct = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "            soft_loss = soft_loss_fct(\n",
    "                student_logits_t, \n",
    "                teacher_logits_t\n",
    "            ) * (temperature ** 2)\n",
    "            \n",
    "            # Combined loss\n",
    "            loss = (1 - alpha) * hard_loss + alpha * soft_loss\n",
    "            \n",
    "            # Scale loss for gradient accumulation\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient accumulation\n",
    "            if (batches + 1) % gradient_accumulation_steps == 0:\n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(student_model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item() * gradient_accumulation_steps\n",
    "            batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': loss.item() * gradient_accumulation_steps,\n",
    "                'avg_loss': total_loss / batches\n",
    "            })\n",
    "            \n",
    "            # Log every 5 batches\n",
    "            if batches % 5 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batches}, Loss: {loss.item() * gradient_accumulation_steps:.4f}\")\n",
    "        \n",
    "        # Make sure to step optimizer at the end of epoch if needed\n",
    "        if batches % gradient_accumulation_steps != 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        avg_loss = total_loss / batches\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} completed, Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint after each epoch\n",
    "        try:\n",
    "            checkpoint_path = checkpoint_dir / f\"student_model_checkpoint_epoch_{epoch+1}.pt\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': student_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save checkpoint: {e}\")\n",
    "        \n",
    "        # Clear cache to free up memory\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Training failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Evaluate the student model after knowledge distillation\n",
    "    print(\"Evaluating student model after knowledge distillation...\")\n",
    "    final_meteor, final_perplexity = evaluate_model(\n",
    "        student_model, \n",
    "        student_tokenizer, \n",
    "        subset, \n",
    "        max_length=max_length,\n",
    "        max_new_tokens=50,\n",
    "        batch_size=4\n",
    "    )\n",
    "    print(f\"Final Student Model - Average METEOR Score: {final_meteor * 100:.2f}%\")\n",
    "    print(f\"Final Student Model - Average Perplexity: {final_perplexity:.2f}\")\n",
    "\n",
    "    # Compare results\n",
    "    print(\"\\nKnowledge Distillation Results Summary:\")\n",
    "    print(f\"METEOR Score: {initial_meteor * 100:.2f}% → {final_meteor * 100:.2f}%\")\n",
    "    print(f\"Perplexity: {initial_perplexity:.2f} → {final_perplexity:.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Final evaluation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Distilled Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
